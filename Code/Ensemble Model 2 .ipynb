{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325bdd60-3815-4e13-979d-8b98ccac924b",
   "metadata": {},
   "source": [
    "# Data 245 - Machine Learning Project \n",
    "\n",
    "# Internet Downtime Prediction Analysis using ML Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132de6c3-3375-4291-834d-7ab16d21d6fc",
   "metadata": {},
   "source": [
    "### Presented By: Group 6 (Bhavik Patel, Poojan Gagrani, Kashish Thakur, Yuti Khamker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e05182a",
   "metadata": {},
   "source": [
    "### Ensemble Model 2 (SVM, Decision tree, Random Forest, XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293c8f3-d237-40f4-b40d-69880377da7a",
   "metadata": {},
   "source": [
    "## 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf2fb4-e09d-4fb2-a84a-30bed2c1f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56134e8d-e992-4f49-8b59-2c2ba80b729f",
   "metadata": {},
   "source": [
    "## 2. Reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079f776-660d-4e3d-9fd3-c1ea89ca8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/bhavikpatel/Desktop/poject MSDA/Data 245/Project/Data/Outage_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f29efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=100000, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b834b-c7e6-4783-9b60-c91c088c988b",
   "metadata": {},
   "source": [
    "## 3. Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc481d-e7b9-47e5-b90d-c3e8bde4f9b8",
   "metadata": {},
   "source": [
    "### Original feature description from data source, Ref: https://wiki.mozilla.org/Mozilla_Network_Outages_Data_Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c0d09b-1a64-487d-847c-265fb68fc446",
   "metadata": {},
   "source": [
    "`country`: the Country code of the client.\n",
    "\n",
    "`city`: the City name (only for cities with a population >= 15000, 'unknown' otherwise).\n",
    "\n",
    "`datetime`: the date and the time (truncated to hour) the data was submitted by the client.\n",
    "\n",
    "`proportion_undefined`: the proportion of users who failed to send telemetry for a reason that was not listed in the other cases.\n",
    "\n",
    "`proportion_timeout`: the proportion of users that had their connection timeout while uploading telemetry (after 90s, in Firefox Desktop).\n",
    "\n",
    "`proportion_abort`: the proportion of users that had their connection terminated by the client (for example, terminating open connections before shutting down).\n",
    "\n",
    "`proportion_unreachable`: the proportion of users that failed to upload telemetry because the server was not reachable (e.g. because the host was not reachable, proxy problems or OS waking up after a suspension).\n",
    "\n",
    "`proportion_terminated`: the proportion of users that had their connection terminated internally by the networking code.\n",
    "\n",
    "`proportion_channel_open`: the proportion of users for which the upload request was terminated immediately, by the client, because of a Necko internal error.\n",
    "\n",
    "`avg_dns_success_time`: the average time it takes for a successful DNS resolution, in milliseconds.\n",
    "\n",
    "`missing_dns_success`: counts how many sessions did not report the `DNS_LOOKUP_TIME` histogram.\n",
    "\n",
    "`avg_dns_failure_time`: the average time it takes for an unsuccessful DNS resolution, in milliseconds.\n",
    "\n",
    "`missing_dns_failure`: counts how many sessions did not report the `DNS_FAILED_LOOKUP_TIME` histogram.\n",
    "\n",
    "`count_dns_failure`: the average count of unsuccessful DNS resolutions reported.\n",
    "\n",
    "`ssl_error_prop`: the proportion of users that reported an error through the `SSL_CERT_VERIFICATION_ERRORS` histogram.\n",
    "\n",
    "`avg_tls_handshake_time`: the average time after the TCP SYN to ready for HTTP, in milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968121d-1250-4a61-99f2-000c76f1329d",
   "metadata": {},
   "source": [
    "### Defining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565e4b8-5434-44e4-ac58-10b039978c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86318d29-73b6-45fe-96b2-727febd99b74",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Showing first 5 values of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500cc3f-c461-467b-afee-186c6f51c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8fbc2-35cf-4f96-9e07-1b375eb7c5a3",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Showing last 5 values of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851d8cf-9fd9-4b84-997d-62569bd44b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b05ec-4e71-46a8-8917-14c782e7a917",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Checking datatypes of the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4e58b-c3e7-49a5-8175-0a0799933531",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d0ab3-5a71-4c7b-ae20-18d66a4739b6",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Showing descriptive statistics of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86567243-1eeb-48c4-aa7b-426127bfe4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027feece-991d-408b-8207-75af978be7b2",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Showing unique values of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058640c-c45c-424b-b1c4-6b089c078e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabced47-b9bd-4da2-baaf-ea93c50ad2d7",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Checking the null values in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4dcd8-f2c9-430a-85e3-93f63ada7cee",
   "metadata": {},
   "source": [
    "## 4. Data Quality Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e04cbb-962b-410f-a61f-ee6978a0cf9f",
   "metadata": {},
   "source": [
    "### Data quality for continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23f4fe-fb8f-4c5a-ad12-fddc168e46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [\n",
    "    'proportion_undefined', 'proportion_timeout', 'proportion_abort',\n",
    "    'proportion_unreachable', 'proportion_terminated', 'proportion_channel_open',\n",
    "    'avg_dns_success_time', 'avg_dns_failure_time', 'count_dns_failure',\n",
    "    'ssl_error_prop', 'avg_tls_handshake_time'\n",
    "]\n",
    "\n",
    "data_quality_report = pd.DataFrame(index=continuous_features)\n",
    "\n",
    "data_quality_report['Count'] = df[continuous_features].count()\n",
    "\n",
    "data_quality_report['Missing Values in %'] = (1 - (df[continuous_features].count() / len(df))) * 100\n",
    "\n",
    "data_quality_report['Cardinality'] = df[continuous_features].nunique()\n",
    "\n",
    "data_quality_report['Minimum'] = df[continuous_features].min()\n",
    "\n",
    "data_quality_report['Quartile 1'] = df[continuous_features].quantile(0.25)\n",
    "\n",
    "data_quality_report['Mean'] = df[continuous_features].mean()\n",
    "\n",
    "data_quality_report['Median'] = df[continuous_features].median()\n",
    "\n",
    "data_quality_report['Quartile 3'] = df[continuous_features].quantile(0.75)\n",
    "\n",
    "data_quality_report['Maximum'] = df[continuous_features].max()\n",
    "\n",
    "data_quality_report['Standard Deviation'] = df[continuous_features].std()\n",
    "\n",
    "data_quality_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631aa96-4ba7-45f9-90de-5a79a47a101f",
   "metadata": {},
   "source": [
    "### Data quality for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e324792-ad73-4ea0-94a5-2f386cd665a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['country', 'city']\n",
    "\n",
    "data_quality_report_categorical = pd.DataFrame(index=categorical_features)\n",
    "\n",
    "data_quality_report_categorical['Count'] = df[categorical_features].count()\n",
    "\n",
    "data_quality_report_categorical['Missing Values in %'] = (1 - (df[categorical_features].count() / len(df))) * 100\n",
    "\n",
    "data_quality_report_categorical['Cardinality'] = df[categorical_features].nunique()\n",
    "\n",
    "data_quality_report_categorical['Mode'] = df[categorical_features].mode().iloc[0]\n",
    "\n",
    "data_quality_report_categorical['Mode Frequency'] = df[categorical_features].apply(lambda x: x.value_counts().iloc[0])\n",
    "\n",
    "data_quality_report_categorical['Mode in %'] = (df[categorical_features].apply(lambda x: x.value_counts().iloc[0]) / len(df)) * 100\n",
    "\n",
    "data_quality_report_categorical['2nd Mode'] = df[categorical_features].apply(lambda x: x.value_counts().index[1] if len(x.value_counts()) > 1 else 'N/A')\n",
    "\n",
    "data_quality_report_categorical['2nd Mode Frequency'] = df[categorical_features].apply(lambda x: x.value_counts().iloc[1] if len(x.value_counts()) > 1 else 'N/A')\n",
    "\n",
    "data_quality_report_categorical['2nd Mode in %'] = (df[categorical_features].apply(lambda x: x.value_counts().iloc[1] if len(x.value_counts()) > 1 else 'N/A') / len(df)) * 100\n",
    "\n",
    "data_quality_report_categorical['3rd Mode'] = df[categorical_features].apply(lambda x: x.value_counts().index[2] if len(x.value_counts()) > 1 else 'N/A')\n",
    "\n",
    "data_quality_report_categorical['3rd Mode Frequency'] = df[categorical_features].apply(lambda x: x.value_counts().iloc[2] if len(x.value_counts()) > 1 else 'N/A')\n",
    "\n",
    "data_quality_report_categorical['3rd Mode in %'] = (df[categorical_features].apply(lambda x: x.value_counts().iloc[2] if len(x.value_counts()) > 1 else 'N/A') / len(df)) * 100\n",
    "\n",
    "data_quality_report_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b97fd-19db-4786-bce3-7258f03d5ae6",
   "metadata": {},
   "source": [
    "## 5. Initial Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598152ab-6de0-49c3-80af-980aa7c9c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "selected_columns = [\n",
    "    'proportion_timeout', 'proportion_abort', 'proportion_unreachable',\n",
    "    'proportion_terminated', 'avg_dns_success_time', 'avg_dns_failure_time',\n",
    "    'count_dns_failure', 'ssl_error_prop', 'avg_tls_handshake_time'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, col in enumerate(selected_columns, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.histplot(df[col], bins=50, kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    if 'proportion' in col:\n",
    "        plt.xlim(0, df[col].max()) \n",
    "    elif 'avg' in col:\n",
    "        plt.xlim(0, df[col].max())\n",
    "    elif 'count' in col:\n",
    "        plt.xlim(0, df[col].max()) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ebf31-c7b1-43b2-ad05-4a22b1859e81",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The histogram above demonstrates the ditribution of the continuous features present in the dataset. Here we can observe that most of the proportion values lie between 0 and 1 and they have some values higher than 1 which can possibly be outliers. However, avg_tls_handshake_time, avg_dns_success_time and avg_dns_failure_time have much higher values present the reason behind this is that they are recorded in milliseconds and could be tranformed, if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8031e90-423b-4749-adbd-14512c4b5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_features = [col for col in df.columns if \"proportion\" in col]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "for i, feature in enumerate(proportion_features, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.boxplot(y=df[feature])\n",
    "    plt.ylim(-0.1, 1.1) \n",
    "    plt.title(feature)\n",
    "    plt.ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff903c7-36e1-4a35-ab63-a4b73d305912",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The boxplots above visualizes the spread of data for proportion of the undefined, timeout, abort, unreachable, terminated, channel_open features respectively. They are necessary to understand our target feature as they're the set of features that captures the outcomes of the telemetry signals captured from the host machines. Here, we can observe that proportion_unreachable has the highest spread of values and also aligns with our target feature requirement as it captures the failure of upload of the telemetary signals indicating the possible outage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326b043-0c4f-448e-b15e-8b1ad80a1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 10))\n",
    "\n",
    "sns.scatterplot(data=df, x='proportion_unreachable', y='proportion_timeout', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Proportion Unreachable vs Timeout')\n",
    "\n",
    "sns.scatterplot(data=df, x='proportion_unreachable', y='proportion_terminated', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Proportion Unreachable vs Terminated')\n",
    "\n",
    "\n",
    "sns.scatterplot(data=df, x='proportion_unreachable', y='proportion_abort', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Proportion Unreachable vs Proportion Abort')\n",
    "\n",
    "sns.scatterplot(data=df, x='proportion_unreachable', y='proportion_channel_open', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Proportion Unreachable vs Proportion Channel Open')\n",
    "\n",
    "sns.scatterplot(data=df, x='proportion_unreachable', y='count_dns_failure', ax=axes[2,0])\n",
    "axes[2,0].set_title('Proportion Unreachable vs Count DNS Failure')\n",
    "\n",
    "sns.scatterplot(data=df, x='proportion_unreachable', y='avg_tls_handshake_time', ax=axes[2,1])\n",
    "axes[2,1].set_title('Proportion Unreachable vs Avg TLS Handshake Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03527d07-635b-4310-8afe-3463a9b15d71",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The scatterplot above demonstrates the spread of the continuous features with respect to proportion_unreachable which is our target feature, this is to understand the correlation and the density of the values lying amongst the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6250d-6820-4fec-aeda-f4c57f3851bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['country'].value_counts().head(15).plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 15 Countries by Data Count')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['city'].value_counts().head(15).plot(kind='bar', color='lightcoral')\n",
    "plt.title('Top 15 Cities by Data Count')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c29711-1f8f-42ce-9f63-f96bc31d02fc",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The barplots above are used to demonstrate the top 15 countries and cities with highest data counts. It's quite significant that the most of the data is captures from United States followed by Germany, France and China. It's important to note that the cities have highest count for unknown as the cities with population less than 15,000 are labelled as unknows as originally mentioned by the data owners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384fe5b-317a-490b-8222-3b64904b3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_numeric_features = [\n",
    "    'proportion_timeout', 'proportion_abort', 'proportion_unreachable',\n",
    "    'proportion_terminated', 'avg_dns_success_time', 'avg_dns_failure_time',\n",
    "    'count_dns_failure', 'ssl_error_prop', 'avg_tls_handshake_time'\n",
    "]\n",
    "\n",
    "\n",
    "correlation_matrix = df[selected_numeric_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a223ec-68a3-4270-8ad9-2e709085ed7c",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "The heatmap above is used to understand the correlation between continuous features in the dataset. We can see that most of the features have positive correlation. However there are some features which have little negative correlation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a43da-23c4-4b63-95d6-c67264f76de8",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f22c3-c6ab-45d3-84ab-5d477d74c88e",
   "metadata": {},
   "source": [
    "### Handling country with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96aa4dc-7f49-4c9c-b4a8-1f4e196f5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_country = df[df['country'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff4e1a-1e0e-430c-8212-a4addfd1338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_country['city'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b667ae-ac08-49ef-861c-8a09f4b79048",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_windhoek_count = df_null_country['city'].value_counts()['Windhoek']\n",
    "city_unknown_count =  df_null_country['city'].value_counts()['unknown']\n",
    "\n",
    "total = (city_windhoek_count + city_unknown_count)\n",
    "\n",
    "print('Windhoek count = {} and unknown count = {}'.format(city_windhoek_count, city_unknown_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6867ac8-9a6e-42f1-b88c-18cbbe434ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"city\"] == \"Windhoek\", \"country\"] = 'NA'\n",
    "df_null_country.loc[df_null_country[\"city\"] == \"Windhoek\", \"country\"] = 'NA'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416541c-376d-4524-ae3f-9da98cb29222",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Assigning the `country` value 'NA' i.e., Namibia where `city` is Windhoek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ac9c0-396d-4793-9c46-956292d976e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74398465-7383-4627-9952-211b562dcac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_country[df_null_country['country'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98749c-ddf5-420f-89bb-f86c7538089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['country'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e5366-70c2-4cdd-81cc-cf4229e1bf5c",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Dropping all the remainder countries having null values as city is also unknown and the data count is significantly small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150dff66-47fe-4e3c-9294-af70a5d21c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40c105-9694-49b3-a8dc-fa5a4dbdbece",
   "metadata": {},
   "source": [
    "### Handling all the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d055d57-b4fc-4603-94ce-f1fa506f4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fde2fb6-13c6-48ac-b257-4e967e4a7e41",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Dropping all the null values as the data count for null values is quite small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcee73c-e4b2-4b8f-b857-b276ab0f4913",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d6302-8acd-4752-9920-bdcf97700541",
   "metadata": {},
   "source": [
    "### Handling city with unknown values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725af96-62ea-48e7-81bc-1b6a0ab1cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_unknown_count =  df['city'].value_counts()['unknown']\n",
    "\n",
    "city_unknown_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed5bcc-cb1c-4f87-b2c9-a902a37cd93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_unknown = df[df['city'] == 'unknown']\n",
    "df_city_unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ea7b90",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4880675",
   "metadata": {},
   "source": [
    "### Define time slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'datetime' to 'hour'\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "# Define time slots\n",
    "def get_detailed_time_slot(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 'Late Night'\n",
    "    elif 6 <= hour < 9:\n",
    "        return 'Early Morning'\n",
    "    elif 9 <= hour < 12:\n",
    "        return 'Late Morning'\n",
    "    elif 12 <= hour < 15:\n",
    "        return 'Early Afternoon'\n",
    "    elif 15 <= hour < 18:\n",
    "        return 'Late Afternoon'\n",
    "    elif 18 <= hour < 21:\n",
    "        return 'Early Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df['time_slot'] = df['hour'].apply(get_detailed_time_slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a19b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56cb007",
   "metadata": {},
   "source": [
    "### Class Labeling (Data discretization) using composite score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example features that might contribute to an internet quality score\n",
    "features = ['proportion_timeout', 'proportion_unreachable', 'proportion_terminated', \n",
    "            'avg_dns_failure_time', 'count_dns_failure']\n",
    "\n",
    "# Create a composite score as a simple sum of standardized features\n",
    "df['composite_score'] = df[features].apply(lambda x: (x - x.mean()) / x.std()).sum(axis=1)\n",
    "\n",
    "# Calculate the quantiles on this composite score\n",
    "quantiles = df['composite_score'].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "# Define the labeling function with the correct quartile values\n",
    "def label_quality(score, quantiles):\n",
    "    if score <= quantiles[0.25]:\n",
    "        return 'good'\n",
    "    elif score <= quantiles[0.50]:\n",
    "        return 'moderate'\n",
    "    elif score <= quantiles[0.75]:\n",
    "        return 'bad'\n",
    "    else:\n",
    "        return 'worse'\n",
    "\n",
    "# Apply the labeling function to each row in your dataframe\n",
    "df['quality_label'] = df['composite_score'].apply(label_quality, quantiles=quantiles)\n",
    "\n",
    "# Map the categorical labels to integers\n",
    "label_map = {'good': 0, 'moderate': 1, 'bad': 2, 'worse': 3}\n",
    "df['quality_label_encoded'] = df['quality_label'].map(label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2898e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7aeb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c074f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe34907",
   "metadata": {},
   "source": [
    "### Checking feature importance using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['quality_label', 'quality_label_encoded', 'datetime','time_slot', 'country', 'city'], axis=1)\n",
    "y = df['quality_label_encoded']\n",
    "\n",
    "# Create the RFE object and rank each pixel\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=10)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# RFE ranking\n",
    "ranking_rfe = rfe.ranking_\n",
    "\n",
    "# To map these rankings back to column names:\n",
    "rfe_dict = dict(zip(X.columns, ranking_rfe))\n",
    "sorted_rfe = sorted(rfe_dict.items(), key=lambda item: item[1])\n",
    "\n",
    "# sorted_rfe now contains features and their RFE ranking, sorted from most to least important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6f670",
   "metadata": {},
   "source": [
    "### Encoding the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode 'country' and 'city'\n",
    "label_encoder_country = LabelEncoder()\n",
    "label_encoder_city = LabelEncoder()\n",
    "ordinal_encoder_time_slot = OrdinalEncoder()\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "df['country_encoded'] = label_encoder_country.fit_transform(df['country'])\n",
    "df['city_encoded'] = label_encoder_city.fit_transform(df['city'])\n",
    "\n",
    "# Assuming 'time_slot' is a categorical variable that you want to encode ordinally\n",
    "df['time_slot_encoded'] = ordinal_encoder_time_slot.fit_transform(df[['time_slot']])\n",
    "\n",
    "# For 'composite_score', first, we need to convert it into quartile bins\n",
    "# Then we'll use ordinal encoding on these bins\n",
    "#data['quality_label_encoded'] = pd.qcut(data['quality_label'], q=4, labels=False)\n",
    "\n",
    "# Drop the original columns that have been encoded\n",
    "df.drop(['country', 'city', 'time_slot', 'quality_label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15531bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8045ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(label_encoder_country, 'label_encoder_country.joblib')\n",
    "joblib.dump(label_encoder_city, 'label_encoder_city.joblib')\n",
    "joblib.dump(ordinal_encoder_time_slot, 'ordinal_encoder_time_slot.joblib')\n",
    "print('all encoder saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937da151",
   "metadata": {},
   "source": [
    "## 8. Splitting the dataset into test, train and validate  sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0841231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features are separated: numerical features that need scaling and categorical encoded features that don't\n",
    "numerical_features = ['proportion_timeout', 'proportion_unreachable', 'proportion_terminated', \n",
    "                      'avg_dns_success_time', 'avg_dns_failure_time', 'count_dns_failure', 'ssl_error_prop']\n",
    "categorical_features = ['country_encoded', 'city_encoded', 'time_slot_encoded']\n",
    "X_numerical = df[numerical_features]\n",
    "X_categorical = df[categorical_features]\n",
    "\n",
    "# Target variable\n",
    "y = df['quality_label_encoded']\n",
    "\n",
    "# Split the data into train+validate and test sets (90-10 split)\n",
    "X_temp_num, X_test_num, y_temp, y_test = train_test_split(X_numerical, y, test_size=0.1, stratify=y, random_state=42, shuffle=True)\n",
    "X_temp_cat, X_test_cat = train_test_split(X_categorical, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "# Further split the train+validate into train and validate sets (89-11 split, approximates to 80-10 of original)\n",
    "X_train_num, X_validate_num, y_train, y_validate = train_test_split(X_temp_num, y_temp, test_size=1/9, stratify=y_temp, random_state=42, shuffle=True)\n",
    "X_train_cat, X_validate_cat = train_test_split(X_temp_cat, test_size=1/9, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fecaf7",
   "metadata": {},
   "source": [
    "### Initialize the StandardScaler for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the numerical part of the training data and transform\n",
    "X_train_num_scaled = scaler.fit_transform(X_train_num)\n",
    "X_validate_num_scaled = scaler.transform(X_validate_num)\n",
    "X_test_num_scaled = scaler.transform(X_test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224f8e0",
   "metadata": {},
   "source": [
    "### Combining scaled continous and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7310f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train_num_scaled, X_train_cat.values), axis=1)\n",
    "X_validate = np.concatenate((X_validate_num_scaled, X_validate_cat.values), axis=1)\n",
    "X_test = np.concatenate((X_test_num_scaled, X_test_cat.values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f611c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data (X_train):\")\n",
    "print(X_train)\n",
    "print(\"\\nShape:\", X_train.shape)\n",
    "\n",
    "print(\"\\nValidation Data (X_validate):\")\n",
    "print(X_validate)\n",
    "print(\"\\nShape:\", X_validate.shape)\n",
    "\n",
    "print(\"\\nTest Data (X_test):\")\n",
    "print(X_test)\n",
    "print(\"\\nShape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde0ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nShape:\", X_train.shape)\n",
    "print(\"\\nShape:\", X_validate.shape)\n",
    "print(\"\\nShape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Continous features scaled shape', X_train_num_scaled.shape)\n",
    "print('Categorical features shape', X_train_cat.shape)\n",
    "print('X train shape', X_train.shape)\n",
    "print('\\nContinous features scaled shape', X_validate_num_scaled.shape)\n",
    "print('Categorical features shape', X_validate_cat.shape)\n",
    "print('X train shape', X_validate.shape)\n",
    "print('\\nContinous features scaled shape', X_test_num_scaled.shape)\n",
    "print('Categorical features shape', X_test_cat.shape)\n",
    "print('X train shape', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ddb1d3",
   "metadata": {},
   "source": [
    "### Checking the count of each class in the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ccea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in the target feature\n",
    "class_counts = y_train.value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "# Decide to use SMOTE based on class distribution\n",
    "# Generally, if any class is less than 10-20% of the majority class, SMOTE might be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4dfd0",
   "metadata": {},
   "source": [
    "### Applying SMOTE to the training set if the classes are imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4643b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SMOTE and resample the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resampled Training Data (X_train_smote):\")\n",
    "print(X_train_smote)\n",
    "print(\"\\nShape:\", X_train_smote.shape)\n",
    "print(\"\\nResampled Target Variable (y_train_smote):\")\n",
    "print(y_train_smote)\n",
    "print(\"\\nShape:\", y_train_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b4d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the distribution of classes after SMOTE\n",
    "unique_smote, counts_smote = np.unique(y_train_smote, return_counts=True)\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(dict(zip(unique_smote, counts_smote)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68650359",
   "metadata": {},
   "source": [
    "## 9. Ensemble Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc0cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the necessary imports and data splitting/preprocessing code provided above\n",
    "\n",
    "# Load the saved models\n",
    "dt_classifier = joblib.load('dt_classifier.joblib')\n",
    "random_forest_model = joblib.load('random_forest_model.pkl')\n",
    "xgb_classifier = joblib.load('xgb_classifier.joblib')\n",
    "svm_model = joblib.load('svm_model.joblib')\n",
    "\n",
    "# Create a voting ensemble of the models\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('dt', dt_classifier), \n",
    "        ('rf', random_forest_model),\n",
    "        ('xgb', xgb_classifier), \n",
    "        ('svm', svm_model)\n",
    "    ],\n",
    "    voting='hard'  # or 'soft'\n",
    ")\n",
    "\n",
    "print('fitting the ensemble model...')\n",
    "# Fit the ensemble model\n",
    "ensemble_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict on training and validation set\n",
    "y_train_pred = ensemble_model.predict(X_train_smote)\n",
    "y_val_pred = ensemble_model.predict(X_validate)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_accuracy = accuracy_score(y_train_smote, y_train_pred)\n",
    "val_accuracy = accuracy_score(y_validate, y_val_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad92bf",
   "metadata": {},
   "source": [
    "### Ploting Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb947cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Training Accuracy', 'Validation Accuracy'], [train_accuracy, val_accuracy], color=['blue', 'green'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Ensemble Model Accuracies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3762087",
   "metadata": {},
   "source": [
    "### Confusion Matrix for Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab525bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Validation Set\n",
    "cm = confusion_matrix(y_validate, y_val_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix - Validation Set')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094e779",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(ensemble_model, 'ensemble_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e037b8",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7429a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_ensemble_model = joblib.load('ensemble_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c9222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ensemble model\n",
    "#ensemble_model = joblib.load('ensemble_model.joblib')\n",
    "\n",
    "# Assuming X_test and y_test are your test datasets\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd73ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print classification report for detailed metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6621ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report for detailed metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7920f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9cd04",
   "metadata": {},
   "source": [
    "## Predicting Internet Outage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e84664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ensemble model\n",
    "ensemble_model = joblib.load('ensemble_model.joblib')\n",
    "\n",
    "def predict_outage(country_input, city_input, time_slot_input):\n",
    "    try:\n",
    "        # Encode the country and city inputs\n",
    "        country_encoded = label_encoder_country.transform([country_input])[0]\n",
    "        city_encoded = label_encoder_city.transform([city_input])[0]\n",
    "    except ValueError:\n",
    "        # Handle the error if the input country or city is not recognized\n",
    "        return \"Country or City not recognized\"\n",
    "\n",
    "    # Convert time_slot_input to hour and then to the corresponding time slot category\n",
    "    time_slot_to_hour = {'Late Night': 0, 'Early Morning': 6, 'Late Morning': 9, \n",
    "                         'Early Afternoon': 12, 'Late Afternoon': 15, \n",
    "                         'Early Evening': 18, 'Night': 21}\n",
    "    hour = time_slot_to_hour.get(time_slot_input, -1)  # Default to -1 if time slot is not recognized\n",
    "\n",
    "    # Check if the time slot is valid\n",
    "    if hour == -1:\n",
    "        return \"Time slot not recognized\"\n",
    "\n",
    "    time_slot_category = get_detailed_time_slot(hour)\n",
    "\n",
    "    # Assuming you have an encoder for time_slot_category\n",
    "    try:\n",
    "        time_slot_encoded = ordinal_encoder_time_slot.transform([[time_slot_category]])[0][0]\n",
    "    except ValueError:\n",
    "        return \"Time slot category not recognized\"\n",
    "\n",
    "        # Default or average values for the remaining features\n",
    "    default_values = {\n",
    "        'proportion_timeout': 1.234079246734771,  # replace with the average or typical value if known\n",
    "        'proportion_unreachable': 1.12382143954133,\n",
    "        'proportion_terminated': 1.54313618470646,\n",
    "        'avg_dns_success_time': 1.123814659476422,\n",
    "        'avg_dns_failure_time': 1.432988650019605,\n",
    "        'count_dns_failure': 1.125727213548191,\n",
    "        'ssl_error_prop': 1.87639002049091\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create a DataFrame for all input features\n",
    "    input_features = pd.DataFrame({\n",
    "        'country_encoded': [country_encoded],\n",
    "        'city_encoded': [city_encoded],\n",
    "        'time_slot_encoded': [time_slot_encoded],\n",
    "        **default_values\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "    # Make a prediction using the ensemble model\n",
    "    numeric_prediction = ensemble_model.predict(input_features)[0]\n",
    "    \n",
    "    # Reverse mapping from integer to string labels\n",
    "    reverse_label_map = {0: 'good', 1: 'moderate', 2: 'bad', 3: 'worse'}\n",
    "    string_prediction = reverse_label_map.get(numeric_prediction, \"Unknown\")\n",
    "\n",
    "    return string_prediction\n",
    "# Example usage\n",
    "print(predict_outage('GB', 'London', 'Late Night'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6572d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
